{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BSD license"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "python 3에서 실행 가능하도록 수정, 한글 해설 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#데이터를 불러오고, 글자-벡터 간 상호 변환 매핑 준비 \n",
    "data = open('input.txt', 'r').read() # 텍스트 파일 로드 [ 디렉토리 내에 input.txt 파일이 있으면 불러올 수 있음.]\n",
    "chars = list(set(data)) # 텍스트 파일에서 고유한 문자 추출 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set => 집합 자료형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순서가 없고 중복을 허용하지 않는다는 특징을 가진다. 중복을 허용하지 않는다는 특징은 프로그래밍에서 매우 유용하게 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 2, 3}\n",
      "{'c', ' ', 'n', 'C', 'o', 'e', 'g', 'r', 'M'}\n"
     ]
    }
   ],
   "source": [
    "# 집합 선언\n",
    "set1 = set([1,2,3])\n",
    "set2 = set(\"Conor Mcgregor\")\n",
    " \n",
    "print(set1)\n",
    "print(set2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2, 3], ['c', ' ', 'n', 'C', 'o', 'e', 'g', 'r', 'M'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set([1,2,3])),list(set(\"Conor Mcgregor\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터는 355개의 글자로 되어 있고, 110개의 고유한 문자가 있습니다.\n",
      "'\\n ,.02‘’가간개것결경계과관교구국그기나는다대던데도됐드듭략러려력련로른를마매면목문미반발변보봉부사살상색서선성수시식안양었에열영외요우위유은을의이인입있장적전점정제주준중지진짓체축출측토통편포표필하한할합해했협호'\n"
     ]
    }
   ],
   "source": [
    "data_size, vocab_size = len(data), len(chars) # 길이\n",
    "print('데이터는 {}개의 글자로 되어 있고, {}개의 고유한 문자가 있습니다.'.format(data_size, vocab_size))\n",
    "print(repr(''.join(sorted(str(x) for x in chars)))) # 추출된 고유한 글자들을 알파벳 순서대로 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### str함수와 repr함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('123', '123')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 123\n",
    "str(a),repr(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Life is too short', \"'Life is too short'\")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"Life is too short\"\n",
    "str(a),repr(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repr은 단일인용부호(')가 좌우로 감싸여진 형태의 문자열을 리턴해 주었다.\n",
    "str의 사용목적 : \"사용자가 보기 쉽게 하기 위해\"\n",
    "\n",
    "repr의 사용목적 : \"문자열로 객체를 다시 생성할 수 있기 위해\"\n",
    "\n",
    "자세한 사항은 https://wikidocs.net/11741 참고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 고유한 글자들(a,b,c,d...)을 숫자(1,2,3,4...)에 매핑하는 사전과, 반대 기능을 수행하는 사전을 만듦\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 73,\n",
       " ' ': 17,\n",
       " ',': 36,\n",
       " '.': 19,\n",
       " '0': 68,\n",
       " '2': 46,\n",
       " '‘': 25,\n",
       " '’': 37,\n",
       " '가': 56,\n",
       " '간': 70,\n",
       " '개': 104,\n",
       " '것': 30,\n",
       " '결': 90,\n",
       " '경': 57,\n",
       " '계': 107,\n",
       " '과': 55,\n",
       " '관': 103,\n",
       " '교': 79,\n",
       " '구': 4,\n",
       " '국': 109,\n",
       " '그': 9,\n",
       " '기': 66,\n",
       " '나': 95,\n",
       " '는': 86,\n",
       " '다': 10,\n",
       " '대': 105,\n",
       " '던': 59,\n",
       " '데': 97,\n",
       " '도': 96,\n",
       " '됐': 2,\n",
       " '드': 67,\n",
       " '듭': 99,\n",
       " '략': 50,\n",
       " '러': 53,\n",
       " '려': 85,\n",
       " '력': 11,\n",
       " '련': 100,\n",
       " '로': 92,\n",
       " '른': 3,\n",
       " '를': 32,\n",
       " '마': 16,\n",
       " '매': 65,\n",
       " '면': 64,\n",
       " '목': 93,\n",
       " '문': 94,\n",
       " '미': 18,\n",
       " '반': 75,\n",
       " '발': 45,\n",
       " '변': 38,\n",
       " '보': 15,\n",
       " '봉': 24,\n",
       " '부': 28,\n",
       " '사': 40,\n",
       " '살': 108,\n",
       " '상': 27,\n",
       " '색': 14,\n",
       " '서': 78,\n",
       " '선': 60,\n",
       " '성': 39,\n",
       " '수': 12,\n",
       " '시': 89,\n",
       " '식': 31,\n",
       " '안': 34,\n",
       " '양': 41,\n",
       " '었': 29,\n",
       " '에': 13,\n",
       " '열': 91,\n",
       " '영': 33,\n",
       " '외': 22,\n",
       " '요': 83,\n",
       " '우': 42,\n",
       " '위': 48,\n",
       " '유': 69,\n",
       " '은': 101,\n",
       " '을': 72,\n",
       " '의': 6,\n",
       " '이': 74,\n",
       " '인': 77,\n",
       " '입': 71,\n",
       " '있': 61,\n",
       " '장': 23,\n",
       " '적': 58,\n",
       " '전': 7,\n",
       " '점': 80,\n",
       " '정': 1,\n",
       " '제': 26,\n",
       " '주': 8,\n",
       " '준': 63,\n",
       " '중': 81,\n",
       " '지': 106,\n",
       " '진': 0,\n",
       " '짓': 5,\n",
       " '체': 52,\n",
       " '축': 21,\n",
       " '출': 44,\n",
       " '측': 82,\n",
       " '토': 54,\n",
       " '통': 62,\n",
       " '편': 76,\n",
       " '포': 35,\n",
       " '표': 84,\n",
       " '필': 102,\n",
       " '하': 87,\n",
       " '한': 88,\n",
       " '할': 98,\n",
       " '합': 43,\n",
       " '해': 20,\n",
       " '했': 47,\n",
       " '협': 49,\n",
       " '호': 51}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_to_ix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### enumerate함수\n",
    "\n",
    "enumerate는 \"열거하다\"라는 뜻. 이 함수는 순서가 있는 자료형(리스트, 튜플, 문자열)을 입력으로 받아 인덱스 값을 포함하는 enumerate 객체를 리턴한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 body\n",
      "1 foo\n",
      "2 bar\n"
     ]
    }
   ],
   "source": [
    "for i, name in enumerate(['body', 'foo', 'bar']):\n",
    "    print(i, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "순서값과 함께 body, foo, bar가 순서대로 출력되었다.\n",
    "\n",
    "즉, 위 예제와 같이 enumerate를 for문과 함께 사용하면 자료형의 현재 순서(index)와 그 값을 쉽게 알 수 있다.\n",
    "\n",
    "for문처럼 반복되는 구간에서 객체가 현재 어느 위치에 있는지 알려주는 인덱스 값이 필요할때 enumerate 함수를 사용하면 매우 유용하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "hidden_size = 100 # hidden state의 뉴런 갯수\n",
    "seq_length = 25 # 학습시킬 때 한번에 불러올 글자 수이자 RNN을 펼쳤을 때의 단계 \n",
    "learning_rate = 1e-1 # 학습속도, 가중치를 조정할 때 이동할 간격\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 모델 파라미터 초기화(가중치는 작은 수의 랜덤한 값, bias는 0으로 초기화)\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden (100,25)\n",
    "#vocab_size : 고유문자 개수 [Wxh : x와 h의 가중치]\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden (100,100)\n",
    "#[Whh : h와 h의 가중치]\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output (25,100)\n",
    "#[Why : h와 y의 가중치]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy의 random 서브패키지에는 난수를 생성하는 명령이 3가지가 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rand: 0부터 1사이의 균일 분포\n",
    "\n",
    "randn: 가우시안 표준 정규 분포\n",
    "\n",
    "randint: 균일 분포의 정수 난수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### rand 명령은 0부터 1사이에서 균일한 확률 분포로 실수 난수를 생성한다.\n",
    "\n",
    "숫자 인수는 생성할 난수의 크기이다. 여러개의 인수를 넣으면 해당 크기를 가진 행렬을 생성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93513181,  0.63723792,  0.45709797,  0.86082746,  0.53882693,\n",
       "        0.16729639,  0.5199929 ,  0.92752859,  0.08082023,  0.63179954])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.14773079,  0.67698661,  0.83980192,  0.62959153,  0.73523859],\n",
       "       [ 0.30682033,  0.9198811 ,  0.84723781,  0.59230736,  0.45690183],\n",
       "       [ 0.35653358,  0.35440135,  0.67222602,  0.99690964,  0.54468561]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.73239222, -1.11319852, -0.15825056, -0.60810003, -1.29147185,\n",
       "        0.6911161 , -0.21898878,  0.41383942,  0.0825576 , -0.47295713])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bh = np.zeros((hidden_size, 1)) # hidden bias (100,1)\n",
    "by = np.zeros((vocab_size, 1)) # output bias (25,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### np.zeros(), np.ones(), np.empty() 함수는 괄호 안에 쓴 숫자 개수만큼의 '0', '1', '비어있는 배열' 공간을 만들어줍니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 일반 rnn 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://ratsgo.github.io/natural%20language%20processing/2017/03/09/rnnlstm/\n",
    "#참고\n",
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "    \"\"\"\n",
    "    inputs, targets는 모두 숫자 인덱스의 리스트이다.\n",
    "    hprev는 H(hidden_size)x1의 array, 이전 학습에서 반환한 마지막 hidden state임.\n",
    "    forward pass(손실값 계산), backward pass(그래디언트 계산)를 모두 수행한 후 \n",
    "    손실값, 각각의 가중치에 대한 그래디언트, 그리고 다음 반복 때 사용할 마지막 hidden state를 반환함\n",
    "    \"\"\"\n",
    "    \n",
    "    xs, hs, ys, ps = {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    loss = 0\n",
    "    \n",
    "    # forward pass(손실값 계산)\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size,1)) # 1-of-k(one-hot) 형태로 변환. 모든 값이 0인 array 준비\n",
    "        xs[t][inputs[t]] = 1 # 해당하는 글자에만 값을 1로 설정 - [0, ..., 0, 1, 0, ..., 0]\n",
    "        hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state 업데이트\n",
    "        #np.dot : Numpy에선 벡터의 내적, 벡터와 행렬의 곱, 행렬곱을 위해 ‘‘대신 ‘dot’함수를 사용합니다.\n",
    "        #         dot’은 Numpy 모듈 함수로서도 배열 객체의 인스턴스 메소드로서도 이용 가능한 함수입니다:\n",
    "        #np.tanh : # TanH 함수: 쌍곡함수 (hyperbolic function)\n",
    "        \n",
    "        ys[t] = np.dot(Why, hs[t]) + by # 다음 글자가 어떤 글자가 나올지에 가능성을 표시한 array(정규화되지 않음)\n",
    "        ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # softmax로 각 글자의 등장 가능성을 확률로 표시\n",
    "        #np.exp() 함수는 밑(base)이 자연상수 e 인 지수함수로 변환해줍니다.\n",
    "        \n",
    "        #softmax는 데이터를 2개 이상의 그룹으로 나누기 위해 binary classification을 확장한 모델이다.\n",
    "        #softmax : http://pythonkim.tistory.com/19  참고\n",
    "        \n",
    "        loss += -np.log(ps[t][targets[t],0]) # cross-entropy를 이용하여 정답과 비교하여 손실값 판정\n",
    "        #cross-entropy는 통계학 용어로, 두 확률 분포 p와 q 사이에 존재하는 정보량을 계산하는 방법을 말한다. \n",
    "\n",
    "        \n",
    "    # backward pass(그래디언트 계산)[역전파]\n",
    "    # 변수 초기화\n",
    "    \n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    #만약 크기를 튜플(tuple)로 명시하지 않고 특정한 배열 혹은\n",
    "    #리스트와 같은 크기의 배열을 생성하고 싶다면 ones_like, zeros_like 명령을 사용한다.\n",
    "    \n",
    "    for t in reversed(range(len(inputs))): #forward pass의 과정을 반대로 진행(t=24부터 시작)\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[t]] -= 1 # y의 그래디언트 계산, softmax 함수의 그래디언트 계산\n",
    "        \n",
    "        \n",
    "        #할당연산자\n",
    "        # -=, +=, *=, /=\n",
    "        # a -= 3 => a = a - 3 \n",
    "        # a += 3 => a = a + 3\n",
    "        # a *= 3 => a = a * 3 \n",
    "        \n",
    "        \n",
    "        dWhy += np.dot(dy, hs[t].T) \n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext # loss에서 사용된 h와 h를 업데이트한 계산의 그래디언트 값을 더함\n",
    "        dhraw = (1 - hs[t] * hs[t]) * dh # tanh 역전파\n",
    "        dbh += dhraw\n",
    "        dWxh += np.dot(dhraw, xs[t].T)\n",
    "        dWhh += np.dot(dhraw, hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "        for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) # 그래디언트 발산 방지\n",
    "            #-> 그래디언트 exploding(발산) 해결 방법 :\n",
    "            #Simple heuristic solution: 발산할 때 마다 작은 수의 그래디언트 clip => 방지.\n",
    "            #특정 임계 값에 도달 할 때마다 다음 알고리즘과 같이 작은 수로 다시 설정됩니다.\n",
    "        \n",
    "        return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "\n",
    "def sample(h, seed_ix, n):\n",
    "    \"\"\"\n",
    "    sample a sequence of integers from the model \n",
    "    h is memory state, seed_ix is seed letter for first time step\n",
    "    모델에서 지정된 글자 수(n) 만큼의 글자(숫자의 리스트)를 출력\n",
    "    h 는 hidden state, seed_ix는 주어진 첫번째 글자\n",
    "    \"\"\"\n",
    "    \n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    x[seed_ix] = 1\n",
    "    ixes = []\n",
    "    for t in range(n):\n",
    "        # forward pass 수행\n",
    "        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "        y = np.dot(Why, h) + by\n",
    "        p = np.exp(y) / np.sum(np.exp(y))\n",
    "\n",
    "        # 샘플링. 임의성을 부여하기 위해 argmax대신 array p에서 주어진 확률에 의해 하나의 문자를 선택\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "\n",
    "        #기존의 데이터에서 샘플링하기        \n",
    "        # numpy.random.choice(a, size=None, replace=True, p=None)\n",
    "        # a : 배열이면 원래의 데이터, 정수이면 range(a) 명령으로 데이터 생성\n",
    "        # size : 정수. 샘플 숫자\n",
    "        # replace : 불리언. True이면 한번 선택한 데이터를 다시 선택 가능\n",
    "        # p : 배열. 각 데이터가 선택될 수 있는 확률\n",
    "\n",
    "\n",
    "        # 다음 글자 추론을 위해 샘플링 된 글자를 다음 입력으로 사용 \n",
    "        x = np.zeros((vocab_size, 1))\n",
    "        x[ix] = 1\n",
    "    \n",
    "        # 결과값 리스트에 추가\n",
    "        ixes.append(ix)\n",
    "    return ixes\n",
    "\n",
    "\n",
    "n, p = 0, 0 #  반복 회수(n) 및 입력 데이터(p) 위치 초기화 \n",
    "\n",
    "# Adagrad 알고리즘에 사용되는 메모리 변수 초기화\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # 학습이 이루어지기 전의 손실값\n",
    "\n",
    "while True:\n",
    "  # 입력데이터 준비, 텍스트의 맨 앞쪽부터 seq_length만큼씩 데이터를 준비\n",
    "  # 데이터를 모두 사용하면 입력 데이터의 맨 처음으로 이동\n",
    "    if p+seq_length+1 >= len(data) or n == 0: \n",
    "        hprev = np.zeros((hidden_size,1)) # RNN 메모리 초기화\n",
    "        p = 0 # 입력 데이터의 맨 처음으로 이동\n",
    "  \n",
    "        # 입력(p~p+24번째 글자), 목표(p+1~p+25번째 글자) 데이터를 준비 \n",
    "        inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "        targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "    # 학습을 100번 반복할 때마다 학습 결과를 출력\n",
    "    if n % 100 == 0:\n",
    "        sample_ix = sample(hprev, inputs[0], 200) #지금까지 학습한 RNN을 이용하여 숫자의 리스트를 출력\n",
    "        txt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "        print('----\\n %s \\n----' % (txt, ))\n",
    "\n",
    "        \n",
    "    # 손실함수에서 손실값과 그래디언트를 함께 계산\n",
    "    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "    smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "    if n % 100 == 0: print('iter %d, loss: %f' % (n, smooth_loss)) # 반복횟수, 손실 출력\n",
    "  \n",
    "\n",
    "\n",
    "    # Adagrad 방식으로 파라미터 업데이트\n",
    "    # Adagrad : 데이터-맞춤 학습속도 조정 방법 중 하나\n",
    "    #\n",
    "    # Assume the gradient dx and parameter vector x\n",
    "    # cache += dx**2           \n",
    "    # x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "    \n",
    "    # Adagrad(Adaptive Gradient)는 변수들을 update할 때 각각의 변수마다 step size를 다르게 설정해서 이동하는 방식이다.\n",
    "    # cache는 그라디언트 벡터의 사이즈와 동일한 사이즈를 갖고 있다\n",
    "    # 변수 eps는 분모가 너무 0에 가깝지 않도록 안정화 역할을 하고 주로 1e-4에서 1e-8의 값이 할당된다.\n",
    "    # Adagrad의 단점이 있다면, 딥러닝의 경우에는, 학습 속도가 단조적이라 너무 한 방향으로 급진적(aggressive)으로 나가거나,\n",
    "    # 혹은 학습을 너무 빨리 멈출 가능성도 있다.\n",
    "\n",
    "    for param, dparam, mem in zip([Wxh,  Whh,  Why,  bh,  by],   # 가중치\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],  # 그래디언트\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]): # 메모리\n",
    "        \n",
    "        #zip 함수는 보통 List여러개로 slice 할때 사용을 합니다.\n",
    "        #>>> zip([1,2,3], [4,5,6])  :   [(1, 4), (2, 5), (3, 6)]\n",
    "        \n",
    "        #zip(\"abc\", \"def\")  :  [('a', 'd'), ('b', 'e'), ('c', 'f')]\n",
    "        \n",
    "        mem += dparam * dparam\n",
    "        param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # 실제 파라메터 업데이트\n",
    "        p += seq_length # 데이터 포인터를 seq_length만큼 우측으로 이동.\n",
    "        n += 1 # 반복횟수 카운터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### LSTM으로 코드 변환 \n",
    "lossFun 부분만 변경.=>vanishing gradient problem 해결\n",
    "\n",
    "LSTM은 RNN의 hidden state에 cell-state를 추가한 구조입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFun(inputs, targets, hprev, cprev):\n",
    "    xs, hs, cs, is_, fs, os, gs, ys, ps= {}, {}, {}, {}, {}, {}, {}, {}, {}\n",
    "    hs[-1] = np.copy(hprev) # t=0일때 t-1 시점의 hidden state가 필요하므로\n",
    "    cs[-1] = np.copy(cprev)\n",
    "    loss = 0\n",
    "    H = hidden_size\n",
    "    # forward pass\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = np.zeros((vocab_size, 1))\n",
    "        xs[t][inputs[t]] = 1\n",
    "        tmp = np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t - 1]) + bh  # hidden state. ###\n",
    "        is_[t] = sigmoid(tmp[:H])  \n",
    "        fs[t] = sigmoid(tmp[H:2 * H])  #Forget gate.\n",
    "        #시그모이드 함수 : 값을 0과 1사이로 만들어 주기 위함. ###\n",
    "        #그 값이 0이라면 이전 상태의 정보는 잊고 1이라면 이전 상태의 정보를 온전히 기억하게 됩니다\n",
    "        os[t] = sigmoid(tmp[2 * H: 3 * H]) #Output gate.\n",
    "        gs[t] = np.tanh(tmp[3 * H:])\n",
    "        #gs[t]의 범위는 -1~1이기 때문에 각각 강도와 방향을 나타낸다고 이해함.\n",
    "        #input gate = is_[t]⊙gs[t] 는 ‘현재 정보를 기억하기’ 위한 게이트. \n",
    "        cs[t] = fs[t] * cs[t-1] + is_[t] * gs[t] #Final memory cell\n",
    "        hs[t] = os[t] * np.tanh(cs[t])  ## hidden state 업데이트 ##\n",
    "\n",
    "        \n",
    "    # compute loss\n",
    "    for i in range(len(targets)):\n",
    "        idx = len(inputs) - len(targets) + i\n",
    "        ys[idx] = np.dot(Why, hs[idx]) + by  # unnormalized log probabilities for next chars\n",
    "        ps[idx] = np.exp(ys[idx]) / np.sum(np.exp(ys[idx]))  # probabilities for next chars\n",
    "        loss += -np.log(ps[idx][targets[i], 0])  # softmax (cross-entropy loss)\n",
    "\n",
    "    # backward pass: compute gradients going backwards\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dhnext, dcnext = np.zeros_like(hs[0]), np.zeros_like(cs[0])\n",
    "    n = 1\n",
    "    a = len(targets) - 1\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        if n > len(targets):\n",
    "            continue\n",
    "        dy = np.copy(ps[t])\n",
    "        dy[targets[a]] -= 1  # backprop into y \n",
    "        # 보통 tanh나 로지스틱 시그모이드를 사용.출력값의 범위를 제한해주면서 전 구간에서 미분 가능하기 때문에 backprop이 잘 적용됨.\n",
    "        dWhy += np.dot(dy, hs[t].T)\n",
    "        dby += dy\n",
    "        dh = np.dot(Why.T, dy) + dhnext  # backprop into h\n",
    "        \n",
    "        ### rnn과 다른 부분\n",
    "        dc = dcnext + (1 - np.tanh(cs[t]) * np.tanh(cs[t])) * dh * os[t]  # backprop through tanh nonlinearity\n",
    "        dcnext = dc * fs[t]\n",
    "        di = dc * gs[t]\n",
    "        df = dc * cs[t-1]\n",
    "        do = dh * np.tanh(cs[t])\n",
    "        dg = dc * is_[t]\n",
    "        \n",
    "        #각각의 활성함수에 대한 로컬 그래디언트를 구해 흘러들어온 그래디언트를 곱해주면 됩니다.\n",
    "        \n",
    "        ddi = (1 - is_[t]) * is_[t] * di\n",
    "        ddf = (1 - fs[t]) * fs[t] * df\n",
    "        ddo = (1 - os[t]) * os[t] * do\n",
    "        ddg = (1 - np.tanh(gs[t]) * np.tanh(gs[t])) * dg\n",
    "        da = np.hstack((ddi.ravel(),ddf.ravel(),ddo.ravel(),ddg.ravel()))\n",
    "        \n",
    "        dWxh += np.dot(da[:,np.newaxis],xs[t].T)\n",
    "        dWhh += np.dot(da[:,np.newaxis],hs[t-1].T)\n",
    "        dbh += da[:, np.newaxis]\n",
    "        dhnext = np.dot(Whh.T, da[:, np.newaxis])\n",
    "        n += 1\n",
    "        a -= 1\n",
    "        \n",
    "    for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "        np.clip(dparam, -5, 5, out=dparam)  # clip : 그래디언트 발산 방지\n",
    "    return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs) - 1], cs[len(inputs) - 1]\n",
    "\n",
    "\n",
    "#3개의 게이트(입력/출력/망각 게이트)는 각자 다른 가중치를 가지고 있으며\n",
    "#그 값으로 각 게이트에 입력으로 들어오는 값을 조절합니다.\n",
    "\n",
    "#forget gate(망각 게이트)의 bias를 1로 설정하는 팁을 활용하면 성능을 증가시킬 수도 있습니다. \n",
    "# (한편 Sutskever는 bias를 5로 설정하라고 권고한 바 있습니다.)\n",
    "\n",
    "\n",
    "\n",
    "##생성(generative) 모델 : input문장과 유사한 새로운 문장을 생성.\n",
    "#참고 : https://ratsgo.github.io/deep%20learning/2017/10/10/RNNsty/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
